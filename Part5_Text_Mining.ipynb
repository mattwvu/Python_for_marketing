{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "154a39ca-9632-43df-bdfb-6a1026074a6c",
   "metadata": {},
   "source": [
    "<h1>Text Mining</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4a1bc-a79a-48bc-b4e2-18111ef6e611",
   "metadata": {},
   "source": [
    "<h3>Natural Language Toolkit</h3>\n",
    "<p><a href =\"https://www.nltk.org/\">https://www.nltk.org/</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82e420d8-e757-4c70-84a3-35bc9558d788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Matt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Matt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Matt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Matt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Matt\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load up libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d6284e-fa43-46c3-b4ac-7c3176889d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Username</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-16</td>\n",
       "      <td>Donald F.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Went up to see the daughter in Morgantown! Wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-03</td>\n",
       "      <td>Marianne D.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The bartender was amazing and he made really g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-29</td>\n",
       "      <td>Brooke T.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Shown are the appetizers of Cheesy Nachos with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-02-17</td>\n",
       "      <td>Sara L.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>We walked in and immediately felt under dresse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>Jen G.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Food was great. Service wonderful. One of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2013-08-15</td>\n",
       "      <td>Jimmy G.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>What a great find in Morgantown! Upscale marti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2015-02-09</td>\n",
       "      <td>Sharon M.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Spent two nights at the Hilton Garden Inn. Cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2016-12-22</td>\n",
       "      <td>S M.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I have come here many many times.  Usually the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2014-10-01</td>\n",
       "      <td>J L.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The owner pulled a fast one and changed happy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2016-05-16</td>\n",
       "      <td>Pete C.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I went to Bartini this past weekend following ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date     Username  Rating  \\\n",
       "1   2023-11-16    Donald F.     5.0   \n",
       "2   2024-05-03  Marianne D.     3.0   \n",
       "3   2023-07-29    Brooke T.     1.0   \n",
       "4   2024-02-17      Sara L.     3.0   \n",
       "5   2024-03-12       Jen G.     5.0   \n",
       "..         ...          ...     ...   \n",
       "115 2013-08-15     Jimmy G.     5.0   \n",
       "116 2015-02-09    Sharon M.     1.0   \n",
       "117 2016-12-22         S M.     3.0   \n",
       "118 2014-10-01         J L.     1.0   \n",
       "119 2016-05-16      Pete C.     5.0   \n",
       "\n",
       "                                                Review  \n",
       "1    Went up to see the daughter in Morgantown! Wan...  \n",
       "2    The bartender was amazing and he made really g...  \n",
       "3    Shown are the appetizers of Cheesy Nachos with...  \n",
       "4    We walked in and immediately felt under dresse...  \n",
       "5    Food was great. Service wonderful. One of the ...  \n",
       "..                                                 ...  \n",
       "115  What a great find in Morgantown! Upscale marti...  \n",
       "116  Spent two nights at the Hilton Garden Inn. Cou...  \n",
       "117  I have come here many many times.  Usually the...  \n",
       "118  The owner pulled a fast one and changed happy ...  \n",
       "119  I went to Bartini this past weekend following ...  \n",
       "\n",
       "[109 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataframe\n",
    "review = pd.read_csv(\"bartini_reviews.csv\")\n",
    "\n",
    "# remove na values\n",
    "\n",
    "review = review.dropna()\n",
    "\n",
    "\n",
    "# change Date to date value\n",
    "\n",
    "review[\"Date\"] = pd.to_datetime(review[\"Date\"])\n",
    "review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fc2425-27d8-4bf7-8472-3bb75a4fc00c",
   "metadata": {},
   "source": [
    "<h3>Tokenize</h3>\n",
    "<p>Splitting the text into individual words or \"tokens\"</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fdc2341-249f-4ae1-8193-f89dc256e626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Username</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-16</td>\n",
       "      <td>Donald F.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Went up to see the daughter in Morgantown! Wan...</td>\n",
       "      <td>[Went, up, to, see, the, daughter, in, Morgant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-03</td>\n",
       "      <td>Marianne D.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The bartender was amazing and he made really g...</td>\n",
       "      <td>[The, bartender, was, amazing, and, he, made, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-29</td>\n",
       "      <td>Brooke T.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Shown are the appetizers of Cheesy Nachos with...</td>\n",
       "      <td>[Shown, are, the, appetizers, of, Cheesy, Nach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-02-17</td>\n",
       "      <td>Sara L.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>We walked in and immediately felt under dresse...</td>\n",
       "      <td>[We, walked, in, and, immediately, felt, under...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>Jen G.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Food was great. Service wonderful. One of the ...</td>\n",
       "      <td>[Food, was, great, ., Service, wonderful, ., O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2013-08-15</td>\n",
       "      <td>Jimmy G.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>What a great find in Morgantown! Upscale marti...</td>\n",
       "      <td>[What, a, great, find, in, Morgantown, !, Upsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2015-02-09</td>\n",
       "      <td>Sharon M.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Spent two nights at the Hilton Garden Inn. Cou...</td>\n",
       "      <td>[Spent, two, nights, at, the, Hilton, Garden, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2016-12-22</td>\n",
       "      <td>S M.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I have come here many many times.  Usually the...</td>\n",
       "      <td>[I, have, come, here, many, many, times, ., Us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2014-10-01</td>\n",
       "      <td>J L.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The owner pulled a fast one and changed happy ...</td>\n",
       "      <td>[The, owner, pulled, a, fast, one, and, change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2016-05-16</td>\n",
       "      <td>Pete C.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I went to Bartini this past weekend following ...</td>\n",
       "      <td>[I, went, to, Bartini, this, past, weekend, fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date     Username  Rating  \\\n",
       "1   2023-11-16    Donald F.     5.0   \n",
       "2   2024-05-03  Marianne D.     3.0   \n",
       "3   2023-07-29    Brooke T.     1.0   \n",
       "4   2024-02-17      Sara L.     3.0   \n",
       "5   2024-03-12       Jen G.     5.0   \n",
       "..         ...          ...     ...   \n",
       "115 2013-08-15     Jimmy G.     5.0   \n",
       "116 2015-02-09    Sharon M.     1.0   \n",
       "117 2016-12-22         S M.     3.0   \n",
       "118 2014-10-01         J L.     1.0   \n",
       "119 2016-05-16      Pete C.     5.0   \n",
       "\n",
       "                                                Review  \\\n",
       "1    Went up to see the daughter in Morgantown! Wan...   \n",
       "2    The bartender was amazing and he made really g...   \n",
       "3    Shown are the appetizers of Cheesy Nachos with...   \n",
       "4    We walked in and immediately felt under dresse...   \n",
       "5    Food was great. Service wonderful. One of the ...   \n",
       "..                                                 ...   \n",
       "115  What a great find in Morgantown! Upscale marti...   \n",
       "116  Spent two nights at the Hilton Garden Inn. Cou...   \n",
       "117  I have come here many many times.  Usually the...   \n",
       "118  The owner pulled a fast one and changed happy ...   \n",
       "119  I went to Bartini this past weekend following ...   \n",
       "\n",
       "                                                tokens  \n",
       "1    [Went, up, to, see, the, daughter, in, Morgant...  \n",
       "2    [The, bartender, was, amazing, and, he, made, ...  \n",
       "3    [Shown, are, the, appetizers, of, Cheesy, Nach...  \n",
       "4    [We, walked, in, and, immediately, felt, under...  \n",
       "5    [Food, was, great, ., Service, wonderful, ., O...  \n",
       "..                                                 ...  \n",
       "115  [What, a, great, find, in, Morgantown, !, Upsc...  \n",
       "116  [Spent, two, nights, at, the, Hilton, Garden, ...  \n",
       "117  [I, have, come, here, many, many, times, ., Us...  \n",
       "118  [The, owner, pulled, a, fast, one, and, change...  \n",
       "119  [I, went, to, Bartini, this, past, weekend, fo...  \n",
       "\n",
       "[109 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review['tokens'] = review['Review'].apply(word_tokenize)\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75334ba7-4523-404e-abc3-a28300d2e914",
   "metadata": {},
   "source": [
    "<h3>Stopwords</h3>\n",
    "<p>Removing common words that do not carry significant meaning</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09414bd0-41d4-4423-9112-384e6df2e3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Username</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-11-16</td>\n",
       "      <td>Donald F.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Went up to see the daughter in Morgantown! Wan...</td>\n",
       "      <td>[Went, see, daughter, Morgantown, !, Wanted, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-03</td>\n",
       "      <td>Marianne D.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The bartender was amazing and he made really g...</td>\n",
       "      <td>[bartender, amazing, made, really, good, drink...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-07-29</td>\n",
       "      <td>Brooke T.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Shown are the appetizers of Cheesy Nachos with...</td>\n",
       "      <td>[Shown, appetizers, Cheesy, Nachos, prime, fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-02-17</td>\n",
       "      <td>Sara L.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>We walked in and immediately felt under dresse...</td>\n",
       "      <td>[walked, immediately, felt, dressed, ,, rolled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>Jen G.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Food was great. Service wonderful. One of the ...</td>\n",
       "      <td>[Food, great, ., Service, wonderful, ., One, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2013-08-15</td>\n",
       "      <td>Jimmy G.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>What a great find in Morgantown! Upscale marti...</td>\n",
       "      <td>[great, find, Morgantown, !, Upscale, martini,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2015-02-09</td>\n",
       "      <td>Sharon M.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Spent two nights at the Hilton Garden Inn. Cou...</td>\n",
       "      <td>[Spent, two, nights, Hilton, Garden, Inn, ., C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2016-12-22</td>\n",
       "      <td>S M.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I have come here many many times.  Usually the...</td>\n",
       "      <td>[come, many, many, times, ., Usually, food, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>2014-10-01</td>\n",
       "      <td>J L.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The owner pulled a fast one and changed happy ...</td>\n",
       "      <td>[owner, pulled, fast, one, changed, happy, hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2016-05-16</td>\n",
       "      <td>Pete C.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I went to Bartini this past weekend following ...</td>\n",
       "      <td>[went, Bartini, past, weekend, following, cous...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date     Username  Rating  \\\n",
       "1   2023-11-16    Donald F.     5.0   \n",
       "2   2024-05-03  Marianne D.     3.0   \n",
       "3   2023-07-29    Brooke T.     1.0   \n",
       "4   2024-02-17      Sara L.     3.0   \n",
       "5   2024-03-12       Jen G.     5.0   \n",
       "..         ...          ...     ...   \n",
       "115 2013-08-15     Jimmy G.     5.0   \n",
       "116 2015-02-09    Sharon M.     1.0   \n",
       "117 2016-12-22         S M.     3.0   \n",
       "118 2014-10-01         J L.     1.0   \n",
       "119 2016-05-16      Pete C.     5.0   \n",
       "\n",
       "                                                Review  \\\n",
       "1    Went up to see the daughter in Morgantown! Wan...   \n",
       "2    The bartender was amazing and he made really g...   \n",
       "3    Shown are the appetizers of Cheesy Nachos with...   \n",
       "4    We walked in and immediately felt under dresse...   \n",
       "5    Food was great. Service wonderful. One of the ...   \n",
       "..                                                 ...   \n",
       "115  What a great find in Morgantown! Upscale marti...   \n",
       "116  Spent two nights at the Hilton Garden Inn. Cou...   \n",
       "117  I have come here many many times.  Usually the...   \n",
       "118  The owner pulled a fast one and changed happy ...   \n",
       "119  I went to Bartini this past weekend following ...   \n",
       "\n",
       "                                                tokens  \n",
       "1    [Went, see, daughter, Morgantown, !, Wanted, s...  \n",
       "2    [bartender, amazing, made, really, good, drink...  \n",
       "3    [Shown, appetizers, Cheesy, Nachos, prime, fil...  \n",
       "4    [walked, immediately, felt, dressed, ,, rolled...  \n",
       "5    [Food, great, ., Service, wonderful, ., One, b...  \n",
       "..                                                 ...  \n",
       "115  [great, find, Morgantown, !, Upscale, martini,...  \n",
       "116  [Spent, two, nights, Hilton, Garden, Inn, ., C...  \n",
       "117  [come, many, many, times, ., Usually, food, st...  \n",
       "118  [owner, pulled, fast, one, changed, happy, hou...  \n",
       "119  [went, Bartini, past, weekend, following, cous...  \n",
       "\n",
       "[109 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "review['tokens'] = review['tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words and \"'\" not in word and \"`\" not in word])\n",
    "review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a3e48-3fd7-44db-9742-07cc923be838",
   "metadata": {},
   "source": [
    "<h3>Remove Punctation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5ecf72-29ff-4393-b877-d49fae3d60e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "review['tokens'] = review['tokens'].apply(lambda x: [word for word in x if word not in string.punctuation])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d50d80-65a7-46d8-81fd-9a4b3d0f42a1",
   "metadata": {},
   "source": [
    "<h3>Lemmatization</h3>\n",
    "<p>Reducing words to their base or root form</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc8f4f6d-a4f8-45be-b8f3-dd272cc1b3d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Matt/nltk_data'\n    - 'C:\\\\Users\\\\Matt\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Matt\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Matt\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Matt\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Matt/nltk_data'\n    - 'C:\\\\Users\\\\Matt\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Matt\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Matt\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Matt\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> 2\u001b[0m review[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mreview\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> 2\u001b[0m review[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m review[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x])\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m----> 2\u001b[0m review[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m review[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [\u001b[43mlemmatizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# match that of the corpus.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1176\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[1;34m(self, root, omw_reader)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe multilingual functions are not available with this Wordnet version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1174\u001b[0m     )\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovenances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43momw_prov\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;66;03m# A cache to store the wordnet data of multiple languages\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang_data \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1285\u001b[0m, in \u001b[0;36mWordNetCorpusReader.omw_prov\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m provdict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1284\u001b[0m provdict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1285\u001b[0m fileids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_omw_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileids\u001b[49m()\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fileid \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[0;32m   1287\u001b[0m     prov, langfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(fileid)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Matt/nltk_data'\n    - 'C:\\\\Users\\\\Matt\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Matt\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Matt\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Matt\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "review['tokens'] = review['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef6cdc5-76d6-4622-8adf-02240649b8cc",
   "metadata": {},
   "source": [
    "<h2>Basic Analysis</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a46ce21-3d66-4a81-8b69-7c9b0411765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84807d8-b36a-4efa-9d0e-32d1ef06a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some libraries\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6633f2-100a-4a7f-b2ad-0f1f6be957e6",
   "metadata": {},
   "source": [
    "<h3>Word Frequency Analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950988d5-f88c-4c1b-8b82-8c09cc8f8e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(word for sublist in review['tokens'] for word in sublist)\n",
    "top_words = word_freq.most_common(10)\n",
    "print(\"Top 10 frequent words:\", top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff8535a-3585-4399-aad8-13d46b616257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "\n",
    "# Visualization of Word Frequencies\n",
    "plt.bar(*zip(*top_words))\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Frequent Words')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3724ff-7bf7-44b6-b587-f6581f399c67",
   "metadata": {},
   "source": [
    "<h3>Vader Sentiment Analyzer</h3>\n",
    "<p><a href =\"https://vadersentiment.readthedocs.io/e\">https://vadersentiment.readthedocs.io/</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c08ce-7e9f-46fa-a8fe-ffa28750329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7761173e-cb82-41c7-86ec-0cb65a84451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze sentiment and return sentiment label\n",
    "def analyze_sentiment(tokens):\n",
    "    text = ' '.join(tokens)\n",
    "    scores = sid.polarity_scores(text)\n",
    "    if scores['compound'] >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif scores['compound'] <= -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509aaf2-916d-499e-afb9-14a1f5be6f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis to each row of the DataFrame\n",
    "review['Opinion'] = review['tokens'].apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28085a8c-d093-4391-8ca3-c12c89b1da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the DataFrame with sentiment analysis results\n",
    "print(review[['Review', 'Opinion']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e57137-8cda-4bfe-803f-14b304408da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar plot for opinion\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(data = review, x = \"Opinion\")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Opinion Sentiment')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4a4cf2-26ce-4852-8d80-53881604dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment analysis to each row of the DataFrame\n",
    "review['Opinion'] = review['tokens'].apply(analyze_sentiment)\n",
    "\n",
    "# Aggregate sentiment over time\n",
    "review['month'] = review['Date'].dt.to_period('M')\n",
    "sentiment_over_time = review.groupby(['month', 'Opinion']).size().unstack(fill_value=0).reset_index()\n",
    "\n",
    "# Convert period to datetime\n",
    "sentiment_over_time['month'] = sentiment_over_time['month'].dt.to_timestamp()\n",
    "\n",
    "# Melt the DataFrame for seaborn compatibility\n",
    "sentiment_melted = sentiment_over_time.melt(id_vars='month', value_vars=['Positive', 'Negative', 'Neutral'], var_name='Sentiment', value_name='Count')\n",
    "\n",
    "# Plot the sentiment trends over time using seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=sentiment_melted, x='month', y='Count', hue='Sentiment', style='Sentiment', s=100)\n",
    "plt.title('Sentiment Analysis Over Time')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a8d4c0-4ab8-4a46-90bc-d46fe7fef317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
